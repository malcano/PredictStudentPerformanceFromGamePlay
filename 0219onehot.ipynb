{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a modified Version of Chris's notebook:https://www.kaggle.com/code/cdeotte/xgboost-baseline-0-676","metadata":{"papermill":{"duration":0.006798,"end_time":"2023-02-17T16:01:07.344033","exception":false,"start_time":"2023-02-17T16:01:07.337235","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score","metadata":{"papermill":{"duration":2.103211,"end_time":"2023-02-17T16:01:09.453124","exception":false,"start_time":"2023-02-17T16:01:07.349913","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T13:04:15.405970Z","iopub.execute_input":"2023-02-19T13:04:15.406380Z","iopub.status.idle":"2023-02-19T13:04:16.022415Z","shell.execute_reply.started":"2023-02-19T13:04:15.406350Z","shell.execute_reply":"2023-02-19T13:04:16.021404Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Train Data and Labels","metadata":{"papermill":{"duration":0.005406,"end_time":"2023-02-17T16:01:09.464559","exception":false,"start_time":"2023-02-17T16:01:09.459153","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/predict-student-performance-from-game-play/train.csv').astype({'level':'int16', 'index': 'int16', 'elapsed_time':'int16','room_coor_x':'float32','room_coor_y':'float32','screen_coor_x':'float32','screen_coor_y':'float32'}).drop(['index','fullscreen','hq','music','name','text'], axis=1)","metadata":{"papermill":{"duration":63.647097,"end_time":"2023-02-17T16:02:13.117300","exception":false,"start_time":"2023-02-17T16:01:09.470203","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-19T13:04:16.024141Z","iopub.execute_input":"2023-02-19T13:04:16.024670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_scale(train):\n#     train.drop(['fullscreen', 'hq', 'music'], axis=1, inplace=True)\n    train['room_coor_x'] = train['room_coor_x'].fillna(0)\n    train['room_coor_y'] = train['room_coor_y'].fillna(0)\n    train['screen_coor_x'] = train['screen_coor_x'].fillna(0)\n    train['screen_coor_y'] = train['screen_coor_y'].fillna(0)\n    train['hover_duration'] = train['hover_duration'].fillna(0)\n    train['page'].fillna(-1)\n    return train\n","metadata":{"papermill":{"duration":0.593918,"end_time":"2023-02-17T16:02:13.717461","exception":false,"start_time":"2023-02-17T16:02:13.123543","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df_scale(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = pd.read_csv('/kaggle/input/predict-student-performance-from-game-play/train_labels.csv')\ntargets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]) )\ntargets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\nprint( targets.shape )\ntargets.head()","metadata":{"papermill":{"duration":0.619152,"end_time":"2023-02-17T16:02:14.342402","exception":false,"start_time":"2023-02-17T16:02:13.723250","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nitems = train['page'].values\nlabels = items.reshape(-1,1)\n\noh_page_encoder = OneHotEncoder()\n\noh_page_encoder.fit(labels)\noh_page_labels = oh_page_encoder.transform(labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\npath = '/kaggle/temp'\nif not os.path.exists(path):\n    os.makedirs(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def oh_page_out(df: pd.DataFrame) -> None:\n\n    train_cat = oh_page_labels.toarray()\n\n    tmp_df = pd.DataFrame(train_cat, columns=[\n                          'page_' + str(col) for col in range(-1, 7)]).astype(dtype = 'int8')\n    pd.concat([df[['session_id', 'level_group']], tmp_df], axis=1).to_csv(\"/kaggle/temp/page_train.csv\", index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh_page_out(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(columns=['page'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineer\nWe create basic aggregate features. Try creating more features to boost CV and LB! The idea for EVENTS feature is from [here][1]\n\n[1]: https://www.kaggle.com/code/kimtaehun/lightgbm-baseline-with-aggregated-log-data","metadata":{"papermill":{"duration":0.006458,"end_time":"2023-02-17T16:02:14.355132","exception":false,"start_time":"2023-02-17T16:02:14.348674","status":"completed"},"tags":[]}},{"cell_type":"code","source":"NUNIQUE = ['event_name','fqid', 'room_fqid']\nMMS = ['elapsed_time', 'hover_duration'] # mean,std\nMEAN = ['level']\nSTD = ['room_coor_x', 'room_coor_y','screen_coor_x','screen_coor_y']\nEVENTS = ['navigate_click','person_click','cutscene_click','object_click',\n          'map_hover','notification_click','map_click','observation_click',\n          'checkpoint']\nPAGE = ['page_-1', 'page_0', 'page_1', 'page_2',\n        'page_3', 'page_4', 'page_5', 'page_6']\n","metadata":{"papermill":{"duration":0.014789,"end_time":"2023-02-17T16:02:14.375968","exception":false,"start_time":"2023-02-17T16:02:14.361179","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text_fqid'].value_counts() # name 6개, event_name 11개, ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer(train):\n    dfs = []\n    for c in NUNIQUE:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('nunique')\n        tmp.name = tmp.name + '_nunique'\n        dfs.append(tmp)\n    # for c in MMS:\n    #     tmp = train.groupby(['session_id','level_group'])[c].agg('mean')\n    #     tmp.name = tmp.name + '_mean'\n    #     dfs.append(tmp)\n    # for c in PAGE:\n    #     tmp = train.groupby(['session_id', 'level_group'])[c].agg('sum')\n    #     tmp.name = tmp.name + '_sum'\n    #     dfs.append(tmp)\n    for c in MMS:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('max')\n        tmp.name = tmp.name + '_max'\n        dfs.append(tmp)\n    for c in MMS:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('min')\n        tmp.name = tmp.name + '_min'\n        dfs.append(tmp)\n    for c in MMS:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('std')\n        tmp.name = tmp.name + '_std'\n        dfs.append(tmp)\n    for c in MEAN:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('mean')\n        tmp.name = tmp.name + '_mean'\n        dfs.append(tmp)\n    for c in STD:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('std')\n        tmp.name = tmp.name + '_std'\n        dfs.append(tmp)\n    for c in EVENTS:\n        train[c] = (train.event_name == c).astype('int8')\n    for c in EVENTS + ['elapsed_time']:\n        tmp = train.groupby(['session_id','level_group'])[c].agg('sum')\n        tmp.name = tmp.name + '_sum'\n        dfs.append(tmp)\n    train = train.drop(EVENTS,axis=1)\n    df = pd.concat(dfs,axis=1)\n    # df = df.fillna(-1)\n    df = df.reset_index()\n    df = df.set_index('session_id')\n    return df","metadata":{"papermill":{"duration":0.021863,"end_time":"2023-02-17T16:02:14.403828","exception":false,"start_time":"2023-02-17T16:02:14.381965","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer_page(train):\n    dfs = []\n    for c in PAGE:\n        tmp = train.groupby(['session_id', 'level_group'])[c].agg('sum')\n        tmp.name = tmp.name + '_sum'\n        dfs.append(tmp)\n\n    df = pd.concat(dfs, axis=1)\n    # df = df.fillna(-1)\n    df = df.reset_index()\n    df = df.set_index('session_id')\n    return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_types = {\"session_id\": 'int64',\"level_group\":'str', \"page_-1\":'int16', \"page_0\":'int16',\"page_1\":'int16',\"page_2\":'int16',\"page_3\":'int16',\"page_4\":'int16',\"page_5\":'int16',\"page_6\":'int16'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = pd.concat([feature_engineer(train), feature_engineer_page(pd.read_csv('/kaggle/temp/page_train.csv', dtype = d_types))], axis=1)\n","metadata":{"papermill":{"duration":60.296336,"end_time":"2023-02-17T16:03:14.706338","exception":false,"start_time":"2023-02-17T16:02:14.410002","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t_col = ['level_group', 'event_name_nunique', 'fqid_nunique',\n       'room_fqid_nunique', 'elapsed_time_max', 'hover_duration_max',\n       'elapsed_time_min', 'hover_duration_min', 'elapsed_time_std',\n       'hover_duration_std', 'level_mean', 'room_coor_x_std',\n       'room_coor_y_std', 'screen_coor_x_std', 'screen_coor_y_std',\n       'navigate_click_sum', 'person_click_sum', 'cutscene_click_sum',\n       'object_click_sum', 'map_hover_sum', 'notification_click_sum',\n       'map_click_sum', 'observation_click_sum', 'checkpoint_sum',\n       'elapsed_time_sum', 'TRASH_level_group', 'page_-1_sum', 'page_0_sum',\n       'page_1_sum', 'page_2_sum', 'page_3_sum', 'page_4_sum', 'page_5_sum',\n       'page_6_sum']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns = t_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop('TRASH_level_group',axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train XGBoost Model\nWe train one model for each of 18 questions. Furthermore, we use data from `level_groups = '0-4'` to train model for questions 1-3, and `level groups '5-12'` to train questions 4 thru 13 and `level groups '13-22'` to train questions 14 thru 18. Because this is the data we get (to predict corresponding questions) from Kaggle's inference API during test inference. We can improve our model by saving a user's previous data from earlier `level_groups` and using that to predict future `level_groups`.","metadata":{"papermill":{"duration":0.007201,"end_time":"2023-02-17T16:03:14.720157","exception":false,"start_time":"2023-02-17T16:03:14.712956","status":"completed"},"tags":[]}},{"cell_type":"code","source":"FEATURES = train.columns[1:]\nprint(f'{len(FEATURES)}개의 특성')\nALL_USERS = train.index.unique()\nprint(f'{len(ALL_USERS)}명의 유저 정보')\n","metadata":{"papermill":{"duration":0.018262,"end_time":"2023-02-17T16:03:14.744741","exception":false,"start_time":"2023-02-17T16:03:14.726479","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.reset_index()\ntrain = train.set_index('session_id')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also use grid search and Bayesian optimization methods","metadata":{"papermill":{"duration":0.006248,"end_time":"2023-02-17T16:03:14.757586","exception":false,"start_time":"2023-02-17T16:03:14.751338","status":"completed"},"tags":[]}},{"cell_type":"code","source":"gkf = GroupKFold(n_splits=2)\noof = pd.DataFrame(data=np.zeros((len(ALL_USERS),18)), index=ALL_USERS)\nmodels = {}\n\n# COMPUTE CV SCORE WITH 5 GROUP K FOLD\nfor i, (train_index, test_index) in enumerate(gkf.split(X=train, groups=train.index)):\n    print('#'*25)\n    print('### Fold',i+1)\n    print('#'*25)\n    \n    xgb_params = {\n    'objective' : 'binary:logistic',\n    'eval_metric':'logloss',\n    'learning_rate': 0.005,\n    'max_depth': 4,\n    'n_estimators': 1500,\n    'early_stopping_rounds': 50,\n    'tree_method':'hist',\n    'subsample':0.8,\n    'colsample_bytree': 0.4,\n    'use_label_encoder' : None}\n    \n    # ITERATE THRU QUESTIONS 1 THRU 18\n    for t in range(1, 19):\n        \n        # USE THIS TRAIN DATA WITH THESE QUESTIONS\n        if t<=3: grp = '0-4'\n        elif t<=13: grp = '5-12'\n        elif t<=22: grp = '13-22'\n            \n        # TRAIN DATA\n        train_x = train.iloc[train_index]\n        train_x = train_x.loc[train_x.level_group == grp]\n        train_users = train_x.index.values\n        train_y = targets.loc[targets.q==t].set_index('session').loc[train_users]\n        \n        # VALID DATA\n        valid_x = train.iloc[test_index]\n        valid_x = valid_x.loc[valid_x.level_group == grp]\n        valid_users = valid_x.index.values\n        valid_y = targets.loc[targets.q==t].set_index('session').loc[valid_users]\n        \n        # TRAIN MODEL        \n        clf =  XGBClassifier(**xgb_params)\n        clf.fit(train_x[FEATURES].astype('float32'), train_y['correct'],\n                eval_set=[ (valid_x[FEATURES].astype('float32'), valid_y['correct']) ],\n                verbose=0)\n        print(f'{t}({clf.best_ntree_limit}), ',end='')\n        \n        # SAVE MODEL, PREDICT VALID OOF\n        models[f'{grp}_{t}'] = clf\n        oof.loc[valid_users, t-1] = clf.predict_proba(valid_x[FEATURES].astype('float32'))[:,1]\n        \n    print()","metadata":{"papermill":{"duration":2234.894611,"end_time":"2023-02-17T16:40:29.658639","exception":false,"start_time":"2023-02-17T16:03:14.764028","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV Score\nWe need to convert prediction probabilities into `1s` and `0s`. The competition metric is F1 Score which is the harmonic mean of precision and recall. Let's find the optimal threshold for `p > threshold` when to predict `1` and when to predict `0` to maximize F1 Score.","metadata":{"papermill":{"duration":0.026858,"end_time":"2023-02-17T16:40:29.713555","exception":false,"start_time":"2023-02-17T16:40:29.686697","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# PUT TRUE LABELS INTO DATAFRAME WITH 18 COLUMNS\ntrue = oof.copy()\nfor k in range(18):\n    # GET TRUE LABELS\n    tmp = targets.loc[targets.q == k+1].set_index('session').loc[ALL_USERS]\n    true[k] = tmp.correct.values","metadata":{"papermill":{"duration":0.108718,"end_time":"2023-02-17T16:40:29.850242","exception":false,"start_time":"2023-02-17T16:40:29.741524","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FIND BEST THRESHOLD TO CONVERT PROBS INTO 1s AND 0s\nscores = []; thresholds = []\nbest_score = 0; best_threshold = 0\n\nfor threshold in np.arange(0.4,0.81,0.01):\n    print(f'{threshold:.02f}, ',end='')\n    preds = (oof.values.reshape((-1))>threshold).astype('int')\n    m = f1_score(true.values.reshape((-1)), preds, average='macro')   \n    scores.append(m)\n    thresholds.append(threshold)\n    if m>best_score:\n        best_score = m\n        best_threshold = threshold","metadata":{"papermill":{"duration":3.952791,"end_time":"2023-02-17T16:40:33.830597","exception":false,"start_time":"2023-02-17T16:40:29.877806","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# PLOT THRESHOLD VS. F1_SCORE\nplt.figure(figsize=(20,5))\nplt.plot(thresholds,scores,'-o',color='blue')\nplt.scatter([best_threshold], [best_score], color='blue', s=300, alpha=1)\nplt.xlabel('Threshold',size=14)\nplt.ylabel('Validation F1 Score',size=14)\nplt.title(f'Threshold vs. F1_Score with Best F1_Score = {best_score:.3f} at Best Threshold = {best_threshold:.3}',size=18)\nplt.show()","metadata":{"papermill":{"duration":0.335778,"end_time":"2023-02-17T16:40:34.195488","exception":false,"start_time":"2023-02-17T16:40:33.859710","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('When using optimal threshold...')\nfor k in range(18):\n        \n    # COMPUTE F1 SCORE PER QUESTION\n    m = f1_score(true[k].values, (oof[k].values>best_threshold).astype('int'), average='macro')\n    print(f'Q{k}: F1 =',m)\n    \n# COMPUTE F1 SCORE OVERALL\nm = f1_score(true.values.reshape((-1)), (oof.values.reshape((-1))>best_threshold).astype('int'), average='macro')\nprint('==> Overall F1 =',m)","metadata":{"papermill":{"duration":0.231439,"end_time":"2023-02-17T16:40:34.456079","exception":false,"start_time":"2023-02-17T16:40:34.224640","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test Data","metadata":{"papermill":{"duration":0.028973,"end_time":"2023-02-17T16:40:34.514201","exception":false,"start_time":"2023-02-17T16:40:34.485228","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# IMPORT KAGGLE API\nimport jo_wilder\nenv = jo_wilder.make_env()\niter_test = env.iter_test()\n\n# CLEAR MEMORY\nimport gc\ndel train, targets, oof, true\n_ = gc.collect()","metadata":{"papermill":{"duration":0.672292,"end_time":"2023-02-17T16:40:35.215446","exception":false,"start_time":"2023-02-17T16:40:34.543154","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"limits = {'0-4': (1, 4), '5-12': (4, 14), '13-22': (14, 19)}\n\ncounter = 0\n\nfor (sample_submission, test) in iter_test:\n    df = df_scale(test)\n\n    df = df.astype({'level': 'int16', 'index': 'int16', 'elapsed_time': 'int16', 'room_coor_x': 'float32', 'room_coor_y': 'float32',\n                       'screen_coor_x': 'float32', 'screen_coor_y': 'float32'}).drop(['index', 'fullscreen', 'hq', 'music', 'name', 'text'], axis=1)\n    if counter == 0:\n        print(sample_submission.head())\n        print(df.head())\n        print(df.shape)\n    oh_page_out(df)\n    print(\"oh_page out df\")\n    print(df.head())\n\n    df = df.drop(columns=['page'], axis=1)\n    print(\"drop page\")\n    print(df.head())\n\n    \n    # FEATURE ENGINEER TEST DATA\n    print(\"start read csv..\")\n    df = pd.concat([feature_engineer(df), feature_engineer_page(\n        pd.read_csv('/kaggle/temp/page_train.csv', dtype=d_types))], axis=1)\n    print(\"read dataframe and concat feature_engineer and feature engineer_page\")\n\n    df.columns = t_col\n    print(\"change df col\")\n    print(df.head())\n\n\n    df = df.drop('TRASH_level_group', axis=1)\n    df = df.reset_index()\n    df = df.set_index('session_id')\n    print(\"drop Trash_level_group!\")\n    print(df.head())\n\n    \n    # INFER TEST DATA\n    grp = test.level_group.values[0]\n    a, b = limits[grp]\n    for t in range(a, b):\n        clf = models[f'{grp}_{t}']\n        p = clf.predict_proba(df[FEATURES].astype('float32'))[:, 1]\n        mask = sample_submission.session_id.str.contains(f'q{t}')\n        sample_submission.loc[mask, 'correct'] = int(p.item() > best_threshold)\n        print(\"sample submission update success!\")\n\n    env.predict(sample_submission)\n    counter += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# limits = {'0-4':(1,4), '5-12':(4,14), '13-22':(14,19)}\n\n# for (sample_submission, test) in iter_test:\n#     test = df_scale(test)\n#     print(test.columns)\n#     test = test.astype({'level':'int16', 'index': 'int16', 'elapsed_time':'int16','room_coor_x':'float32','room_coor_y':'float32','screen_coor_x':'float32','screen_coor_y':'float32'}).drop(['index','fullscreen','hq','music','name','text'], axis=1)\n#     oh_page_out(test)\n#     test = test.drop(columns=['page'],axis=1)\n    \n#     test = test.drop('TRASH_level_group',axis=1)\n#     test = test.reset_index()\n#     test = test.set_index('session_id')\n#     test.columns = t_col\n\n#     # FEATURE ENGINEER TEST DATA\n#     test = pd.concat([feature_engineer(test), feature_engineer_page(pd.read_csv('/kaggle/temp/page_train.csv'))], axis=1)\n    \n#     # INFER TEST DATA\n#     grp = test.level_group.values[0]\n#     a,b = limits[grp]\n#     for t in range(a,b):\n#         clf = models[f'{grp}_{t}']\n#         p = clf.predict_proba(df[FEATURES].astype('float32'))[:,1]\n#         mask = sample_submission.session_id.str.contains(f'q{t}')\n#         sample_submission.loc[mask,'correct'] = int(p.item()>best_threshold)\n    \n#     env.predict(sample_submission)","metadata":{"papermill":{"duration":0.92221,"end_time":"2023-02-17T16:40:36.167111","exception":false,"start_time":"2023-02-17T16:40:35.244901","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA submission.csv","metadata":{"papermill":{"duration":0.02884,"end_time":"2023-02-17T16:40:36.225644","exception":false,"start_time":"2023-02-17T16:40:36.196804","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('submission.csv')\nprint( df.shape )\ndf.info()","metadata":{"papermill":{"duration":0.161589,"end_time":"2023-02-17T16:40:36.416609","exception":false,"start_time":"2023-02-17T16:40:36.255020","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.correct.mean())","metadata":{"papermill":{"duration":0.039949,"end_time":"2023-02-17T16:40:36.485681","exception":false,"start_time":"2023-02-17T16:40:36.445732","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}